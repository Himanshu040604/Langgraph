import os
from dotenv import load_dotenv
from langchain_core.messages import AIMessage, HumanMessage
from langchain_core.tools import tool
from langchain_google_genai import ChatGoogleGenerativeAI

load_dotenv(r"C:\Users\KIIT\Desktop\Ai agents\langgraph\.env")

# STEP 1: Tool Creation, The docstring is important here; as it becomes the "schema" the model reads.
@tool
def get_weather(city: str):
    """Consult this tool to get the current weather for a specific city."""
    # In a real scenario, this would call an API
    return f"The weather in {city} is currently 25Â°C and sunny."


# --- STEP 2: Tool Binding ---
llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=0)
# This connects the tool schema to the Gemini model
llm_with_tools = llm.bind_tools([get_weather])

# Prepare the conversation history
messages = [
    HumanMessage(content="What is the weather like in Delhi?", name="Himanshu")
]

# --- STEP 3: Tool Calling (The AI Decides) ---
# The model sees the user wants weather and sees a weather tool available.
result = llm_with_tools.invoke(messages)

# --- STEP 4: Tool Execution (Manual or via LangGraph) ---
# Note: result.tool_calls contains the "arguments" generated by the AI
if result.tool_calls:
    print("The Model wants to call a tool:")
    for tool_call in result.tool_calls:
        # Actually run the python function with the AI's arguments
        observation = get_weather.invoke(tool_call)
        print(f"Tool Output: {observation}")
else:
    print(result.content)